{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"<p>Star</p> <p></p> Kubernetes Controller Cleaner","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"#what-is-k8s-cleaner","title":"What is k8s-cleaner?","text":"<p>The Kubernetes controller Cleaner (k8s-cleaner) identifies, removes, or updates stale/orphaned or unhealthy resources to maintain a clean and efficient Kubernetes cluster.</p> <p>It is designed to handle any Kubernetes resource types (including custom Kubernetes resources) and provides sophisticated filtering capabilities, including label-based selection and custom Lua-based criteria.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"#features-and-capabilities","title":"Features and Capabilities","text":"<p>1\ufe0f\u20e3 Schedule: Specify the frequency at which the k8s-cleaner should scan the cluster and identify stale resources. Utilise the Cron syntax to define recurring schedules.</p> <p>2\ufe0f\u20e3 DryRun: Enable safe testing of the k8s-cleaner filtering logic without affecting actual resource configurations. Resources matching the criteria will get identified, but no changes will get applied.</p> <p>3\ufe0f\u20e3 Label Filtering: Select resources based on user-defined labels, filtering out unwanted or outdated components. Refine the selection based on label key, operation (equal, different, etc.), and value.</p> <p>4\ufe0f\u20e3 Lua-based Selection Criteria: Leverage the Lua scripting language to create complex and dynamic selection criteria, catering to specific resource management needs. Define custom logic to identify and handle stale resources.</p> <p>5\ufe0f\u20e3 Notifications: Stay informed! The k8s-cleaner keeps users in the loop about every cleaned-up resource, whether removed or optimized. Get detailed notification lists and pick your preferred channel: Slack, Webex, Discord, Telegram, Teams or reports.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"#benefits","title":"Benefits","text":"<p>\ud83d\udcaa Resource Removal: Efficiently remove stale resources from your cluster, reclaiming unused resources and improving resource utilisation.</p> <p>\ud83d\udcaa Resource Updates: Update outdated resources to ensure they align with the latest configurations and maintain consistent functionality.</p> <p>\ud83d\udcaa Reduced Resource Bloat: Minimize resource bloat and maintain a clean and organized cluster, improving overall performance and stability.</p> <p>By combining the flexibility of scheduling, the accuracy of label filtering, the power of Lua-based criteria, and the ability to remove or update stale resources, the k8s-cleaner empowers users to effectively manage Kubernetes environments and optimise resource usage.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"#contribute","title":"Contribute","text":"<p>We encourage everyone to contribute to the example directory by adding new Cleaner configurations \ud83d\udca1. This will help the community benefit from different expertise and build a stronger knowledge base of the Cleaner use cases.</p> <p>\ud83e\udd1d Together we can expand the range of Cleaner applications and make it an even more valuable tool for managing Kubernetes resources efficiently.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"#support-us","title":"Support us","text":"<p>If you like the project, please give us a  if you haven't done so yet. Your support means a lot to us. Thank you .</p> <p> k8s-cleaner</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/deployment/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/deployment/#introduction","title":"Introduction","text":"<p>There is an easy way to identify unhealthy Kubernetes Deployments. An unhealthy Deployment can be one that has Pods with may restarts.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/deployment/#example-scale-down-high-restart-deployment","title":"Example - Scale Down High Restart Deployment","text":"<p>The below Cleaner definition will find all unhealthy Deployments instances. </p> <p>A Deployment instance is considered unhealthy if each of its Pods have been restarted at least 50 times.</p> <p>Any unhealthy deployment will be transformed by scaling its replicas to zero.</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: scale-down-high-restart-deployment\nspec:\n  schedule: \"* 0 * * *\"\n  action: Transform\n  transform: |\n    -- Set replicas to 0\n    function transform()\n      hs = {}\n      obj.spec.replicas = 0\n      hs.resource = obj\n      return hs\n    end  \n  resourcePolicySet:\n    resourceSelectors:\n    - kind: Deployment\n      group: apps\n      version: v1\n    - kind: Pod\n      group: \"\"\n      version: v1\n    aggregatedSelection: |\n      -- any pod restarted more than this value is considered unhealthy\n      min_restarts = 50\n\n      deployment_pods_map = {}\n\n      function getKey(namespace, name)\n    return namespace .. \":\" .. name\n      end\n\n      -- Given a ReplicaSet name, returns the deployment name\n      function getDeploymentName(replicaSetName)\n    local reversed = string.reverse(replicaSetName)\n    local firstDashIndex, _ = string.find(reversed, \"-\")\n    if firstDashIndex then\n       -- Convert index from reversed string to original string\n      local lastDashIndex = #replicaSetName - firstDashIndex + 1\n      return replicaSetName:sub(1, lastDashIndex - 1)\n    else\n      return replicaSetName\n    end\n      end\n\n      -- If Pod's OwnerReference is a ReplicaSet\n      -- returns deployment name. Returns an empty string otherwise\n      function getDeployment(pod)\n    if pod.metadata.ownerReferences ~= nil then\n      for _, owner in ipairs(pod.metadata.ownerReferences) do\n        if owner.kind == \"ReplicaSet\" and owner.apiVersion == \"apps/v1\" then\n          local ownerName = owner.name \n          return getDeploymentName(ownerName)\n        end\n      end\n    end\n    return \"\"\n      end\n\n      -- Function to add a pod to a deployment's pod list\n      function add_pod_to_deployment(depl_key, pod)\n    -- Get the existing pod list for the deployment (or create an empty list if it doesn't exist)\n    pods = deployment_pods_map[depl_key] or {}\n\n    -- Append the new pod to the list\n    table.insert(pods, pod)\n    -- Update the map with the modified pod list\n    deployment_pods_map[depl_key] = pods\n      end\n\n      -- Returns true if \n      function isPodUnhealthy(pod)\n    if pod.status ~= nil and pod.status.containerStatuses ~= nil then\n      for _,container_status in ipairs (pod.status.containerStatuses) do\n        if container_status.restartCount &gt; min_restarts then\n          return true\n        end\n      end\n    end\n    return false\n      end\n\n      function isDeploymentUnhealthy(deployment)\n    depl_key = getKey(deployment.metadata.namespace, deployment.metadata.name)\n    pods = deployment_pods_map[depl_key] or {}\n    for _,pod in ipairs (pods) do\n      if not isPodUnhealthy(pod) then\n        return false\n      end\n    end\n\n    return true\n      end\n\n      function evaluate()\n    local hs = {}\n\n    local deployments = {}\n\n    -- Separate pods from deployments\n    for _, resource in ipairs(resources) do\n      local kind = resource.kind\n      if kind == \"Deployment\" then\n        table.insert(deployments, resource)\n      else\n        deplName = getDeployment(resource)\n        if deplName ~= \"\" then\n          depl_key = getKey(resource.metadata.namespace, deplName)\n          add_pod_to_deployment(depl_key, resource)\n        end  \n      end\n    end\n\n    local unhealthy_deployments = {}\n\n    for _, deployment in ipairs(deployments) do\n      isUnhealthy = isDeploymentUnhealthy(deployment, references)\n      if isUnhealthy then\n        table.insert(unhealthy_deployments, {resource = deployment})\n      end\n    end\n\n    if #unhealthy_deployments &gt; 0 then\n      hs.resources = unhealthy_deployments\n    end\n    return hs\n      end   \n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/deployment/#example-pods-with-high-restarts","title":"Example - Pods with High Restarts","text":"<p>The below Cleaner instance identifies and removes unhealthy Deployments in a Kubernetes cluster based on pod restarts.</p> <p>Unhealthy is a Deployment that has Pods restarted at least 50 times.</p> <p>When an unhealthy Deployment is found, it will be automatically scaled down to zero replicas.</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: scale-down-high-restart-deployment\nspec:\n  schedule: \"* 0 * * *\"\n  action: Transform\n  transform: |\n    -- Set replicas to 0\n    function transform()\n      hs = {}\n      obj.spec.replicas = 0\n      hs.resource = obj\n      return hs\n    end  \n  resourcePolicySet:\n    resourceSelectors:\n    - kind: Deployment\n      group: apps\n      version: v1\n    - kind: Pod\n      group: \"\"\n      version: v1\n    aggregatedSelection: |\n      -- any pod restarted more than this value is considered unhealthy\n      min_restarts = 50\n\n      deployment_pods_map = {}\n\n      function getKey(namespace, name)\n    return namespace .. \":\" .. name\n      end\n\n      -- Given a ReplicaSet name, returns the deployment name\n      function getDeploymentName(replicaSetName)\n    local reversed = string.reverse(replicaSetName)\n    local firstDashIndex, _ = string.find(reversed, \"-\")\n    if firstDashIndex then\n       -- Convert index from reversed string to original string\n      local lastDashIndex = #replicaSetName - firstDashIndex + 1\n      return replicaSetName:sub(1, lastDashIndex - 1)\n    else\n      return replicaSetName\n    end\n      end\n\n      -- If Pod's OwnerReference is a ReplicaSet\n      -- returns deployment name. Returns an empty string otherwise\n      function getDeployment(pod)\n    if pod.metadata.ownerReferences ~= nil then\n      for _, owner in ipairs(pod.metadata.ownerReferences) do\n        if owner.kind == \"ReplicaSet\" and owner.apiVersion == \"apps/v1\" then\n          local ownerName = owner.name \n          return getDeploymentName(ownerName)\n        end\n      end\n    end\n    return \"\"\n      end\n\n      -- Function to add a pod to a deployment's pod list\n      function add_pod_to_deployment(depl_key, pod)\n    -- Get the existing pod list for the deployment (or create an empty list if it doesn't exist)\n    pods = deployment_pods_map[depl_key] or {}\n\n    -- Append the new pod to the list\n    table.insert(pods, pod)\n    -- Update the map with the modified pod list\n    deployment_pods_map[depl_key] = pods\n      end\n\n      -- Returns true if \n      function isPodUnhealthy(pod)\n    if pod.status ~= nil and pod.status.containerStatuses ~= nil then\n      for _,container_status in ipairs (pod.status.containerStatuses) do\n        if container_status.restartCount &gt; min_restarts then\n          return true\n        end\n      end\n    end\n    return false\n      end\n\n      function isDeploymentUnhealthy(deployment)\n    depl_key = getKey(deployment.metadata.namespace, deployment.metadata.name)\n    pods = deployment_pods_map[depl_key] or {}\n    for _,pod in ipairs (pods) do\n      if not isPodUnhealthy(pod) then\n        return false\n      end\n    end\n\n    return true\n      end\n\n      function evaluate()\n    local hs = {}\n\n    local deployments = {}\n\n    -- Separate pods from deployments\n    for _, resource in ipairs(resources) do\n      local kind = resource.kind\n      if kind == \"Deployment\" then\n        table.insert(deployments, resource)\n      else\n        deplName = getDeployment(resource)\n        if deplName ~= \"\" then\n          depl_key = getKey(resource.metadata.namespace, deplName)\n          add_pod_to_deployment(depl_key, resource)\n        end  \n      end\n    end\n\n    local unhealthy_deployments = {}\n\n    for _, deployment in ipairs(deployments) do\n      isUnhealthy = isDeploymentUnhealthy(deployment, references)\n      if isUnhealthy then\n        table.insert(unhealthy_deployments, {resource = deployment})\n      end\n    end\n\n    if #unhealthy_deployments &gt; 0 then\n      hs.resources = unhealthy_deployments\n    end\n    return hs\n      end   \n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/more_examples/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/more_examples/#more-examples","title":"More Examples","text":"<p>The k8s-cleaner has many more examples to explore here.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/more_examples/#contribute-to-k8s-cleaner-examples","title":"Contribute to k8s-cleaner Examples","text":"<p>We encourage all users to contribute to the example directory by adding their own Cleaner configurations\ud83d\udca1. This will help the community benefit from the expertise and build a stronger knowledge base of Cleaner use cases.</p> <p>To add an example, create a new file in the example directory with a descriptive name and add it in the Cleaner configuration. Once the example is added, feel free to submit a Pull Request (PR) to share it with the community.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/outdated_secret_data/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/outdated_secret_data/#introduction","title":"Introduction","text":"<p>There is an easy way to identify unhealthy Kubernetes resources with pods with outdated secrets.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/outdated_secret_data/#example-pod-with-outdated-secret-data","title":"Example - Pod with Outdated Secret Data","text":"<p>The below Cleaner instance finds all Pods in all namespaces mounting Secrets. It identifies and reports any pods that are accessing Secrets that have been modified since the Pod's creation.</p> <p>This is crucial for maintaining data integrity and security, as it prevents Pods from accessing potentially outdated or compromised secrets.</p> <p>By identifying and reporting pods that are accessing modified secrets, the Cleaner instance helps to safeguard applications and sensitive data.</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: list-pods-with-outdated-secret-data\nspec:\n  action: Scan \n  schedule: \"0 * * * *\"\n  notifications:\n  - name: report\n    type: CleanerReport\n  resourcePolicySet:\n    resourceSelectors:\n    - kind: Pod\n      group: \"\"\n      version: v1\n    - kind: Secret\n      group: \"\"\n      version: v1\n    aggregatedSelection: |\n      function getKey(namespace, name)\n        return namespace .. \":\" .. name\n      end\n\n      --  Convert creationTimestamp \"2023-12-12T09:35:56Z\"\n      function convertTimestampString(timestampStr)\n        local convertedTimestamp = string.gsub(\n          timestampStr,\n          '(%d+)-(%d+)-(%d+)T(%d+):(%d+):(%d+)Z',\n          function(y, mon, d, h, mi, s)\n            return os.time({\n              year = tonumber(y),\n              month = tonumber(mon),\n              day = tonumber(d),\n              hour = tonumber(h),\n              min = tonumber(mi),\n              sec = tonumber(s)\n            })\n          end\n        )\n        return convertedTimestamp\n      end\n\n      function getLatestTime(times)\n        local latestTime = nil\n        for _, time in ipairs(times) do\n          if latestTime == nil or os.difftime(tonumber(time), tonumber(latestTime)) &gt; 0 then\n            latestTime = time\n          end\n        end\n        return latestTime\n      end\n\n      function getSecretUpdateTime(secret)\n        local times = {}\n        if secret.metadata.managedFields ~= nil then\n          for _, mf in ipairs(secret.metadata.managedFields) do\n            if mf.time ~= nil then\n              table.insert(times, convertTimestampString(mf.time))\n            end\n          end\n        end\n\n        return getLatestTime(times)\n      end\n\n      function isPodOlderThanSecret(podTimestamp, secretTimestamp)\n        timeDifference = os.difftime(tonumber(podTimestamp), tonumber(secretTimestamp))\n        return  timeDifference &lt; 0\n      end\n\n      function getPodTimestamp(pod)\n        if pod.status ~= nil and pod.status.conditions ~= nil then\n          for _,condition in ipairs(pod.status.conditions) do\n            if condition.type == \"PodReadyToStartContainers\" and condition.status == \"True\" then\n              return convertTimestampString(condition.lastTransitionTime)\n            end\n          end\n        end\n        return convertTimestampString(pod.metadata.creationTimestamp)\n      end\n\n      function hasOutdatedSecret(pod, secrets)\n        podTimestamp = getPodTimestamp(pod)\n\n        if pod.spec.containers ~= nil then\n          for _, container in ipairs(pod.spec.containers) do\n\n            if container.env ~= nil then\n              for _, env in ipairs(container.env) do\n                if env.valueFrom ~= nil and env.valueFrom.secretKeyRef ~= nil then\n                  key = getKey(pod.metadata.namespace, env.valueFrom.secretKeyRef.name)\n                  if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                    return true, \"secret \" .. key .. \" has been updated after pod creation\"\n                  end\n                end\n              end\n            end  \n\n            if  container.envFrom ~= nil then\n              for _, envFrom in ipairs(container.envFrom) do\n                if envFrom.secretRef ~= nil then\n                  key = getKey(pod.metadata.namespace, envFrom.secretRef.name)\n                  if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                    return true, \"secret \" .. key .. \" has been updated after pod creation\"\n                  end\n                end\n              end  \n            end\n          end\n        end\n\n        if pod.spec.initContainers ~= nil then\n          for _, initContainer in ipairs(pod.spec.initContainers) do\n            if initContainer.env ~= nil then\n              for _, env in ipairs(initContainer.env) do\n                if env.valueFrom ~= nil and env.valueFrom.secretKeyRef ~= nil then\n                  key = getKey(pod.metadata.namespace, env.valueFrom.secretKeyRef.name)\n                  if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                    return true, \"secret \" .. key .. \" has been updated after pod creation\"\n                  end\n                end\n              end\n            end\n          end\n        end\n\n        if pod.spec.volumes ~= nil then  \n          for _, volume in ipairs(pod.spec.volumes) do\n            if volume.secret ~= nil then\n              key = getKey(pod.metadata.namespace, volume.secret.secretName)\n              if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                return true, \"secret \" .. key .. \" has been updated after pod creation\"\n              end\n            end\n\n            if volume.projected ~= nil and volume.projected.sources ~= nil then\n              for _, projectedResource in ipairs(volume.projected.sources) do\n                if projectedResource.secret ~= nil then\n                  key = getKey(pod.metadata.namespace, projectedResource.secret.name)\n                  if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                    return true, \"secret \" .. key .. \" has been updated after pod creation\"\n                  end\n                end\n              end\n            end\n          end\n        end\n\n        return false\n      end      \n\n      function evaluate()\n        local hs = {}\n        hs.message = \"\"\n\n        local pods = {}\n        local secrets = {}\n\n        -- Separate secrets and pods\n        for _, resource in ipairs(resources) do\n          local kind = resource.kind\n          if kind == \"Secret\" then\n            key = getKey(resource.metadata.namespace, resource.metadata.name)\n            updateTimestamp = getSecretUpdateTime(resource)\n            secrets[key] = updateTimestamp\n          elseif kind == \"Pod\" then\n            table.insert(pods, resource)\n          end\n        end\n\n        local podsWithOutdatedSecret = {}\n\n        for _, pod in ipairs(pods) do\n          outdatedData, message = hasOutdatedSecret(pod, secrets)\n          if outdatedData then\n            table.insert(podsWithOutdatedSecret, {resource= pod, message = message})\n          end\n        end\n\n        if #podsWithOutdatedSecret &gt; 0 then\n          hs.resources = podsWithOutdatedSecret\n        end\n        return hs\n      end\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/pod_expired_certs/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/pod_expired_certs/#introduction","title":"Introduction","text":"<p>There is an easy way to identify unhealthy Kubernetes resources with pods with expired certificates.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unhealthy_resources/pod_expired_certs/#example-pod-with-outdated-secret-data","title":"Example - Pod with Outdated Secret Data","text":"<p>The below Cleaner instance finds all Pods in all namespaces mounting Secrets containing a Certificate issued by <code>cert-manager</code>.</p> <p>The Cleaner instance identifies and reports any Pod that is using expired certificates.</p> <p>A Pod is using an expired certificates if the secret with certificate have been modified since the Pod's creation.</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: list-pods-with-expired-certificates\nspec:\n  action: Scan \n  schedule: \"0 * * * *\"\n  notifications:\n  - name: report\n    type: CleanerReport\n  resourcePolicySet:\n    resourceSelectors:\n    - kind: Pod\n      group: \"\"\n      version: v1\n    - kind: Secret\n      group: \"\"\n      version: v1\n    - kind: Certificate\n      group: \"cert-manager.io\"\n      version: \"v1\" \n    aggregatedSelection: |\n      function getKey(namespace, name)\n        return namespace .. \":\" .. name\n      end\n\n      --  Convert creationTimestamp \"2023-12-12T09:35:56Z\"\n      function convertTimestampString(timestampStr)\n        local convertedTimestamp = string.gsub(\n          timestampStr,\n          '(%d+)-(%d+)-(%d+)T(%d+):(%d+):(%d+)Z',\n          function(y, mon, d, h, mi, s)\n            return os.time({\n              year = tonumber(y),\n              month = tonumber(mon),\n              day = tonumber(d),\n              hour = tonumber(h),\n              min = tonumber(mi),\n              sec = tonumber(s)\n            })\n          end\n        )\n        return convertedTimestamp\n      end\n\n      function getLatestTime(times)\n        local latestTime = nil\n        for _, time in ipairs(times) do\n          if latestTime == nil or os.difftime(tonumber(time), tonumber(latestTime)) &gt; 0 then\n            latestTime = time\n          end\n        end\n        return latestTime\n      end\n\n      function getSecretUpdateTime(secret)\n        local times = {}\n        if secret.metadata.managedFields ~= nil then\n          for _, mf in ipairs(secret.metadata.managedFields) do\n            if mf.time ~= nil then\n              table.insert(times, convertTimestampString(mf.time))\n            end\n          end\n        end\n\n        return getLatestTime(times)\n      end\n\n      function isPodOlderThanSecret(podTimestamp, secretTimestamp)\n        timeDifference = os.difftime(tonumber(podTimestamp), tonumber(secretTimestamp))\n        return  timeDifference &lt; 0\n      end\n\n      function getPodTimestamp(pod)\n        if pod.status ~= nil and pod.status.conditions ~= nil then\n          for _,condition in ipairs(pod.status.conditions) do\n            if condition.type == \"PodReadyToStartContainers\" and condition.status == \"True\" then\n              return convertTimestampString(condition.lastTransitionTime)\n            end\n          end\n        end\n        return convertTimestampString(pod.metadata.creationTimestamp)\n      end\n\n      -- secrets contains key:value where key identify a Secret with a Certificate and value\n      -- if the time of latest update\n      function hasOutdatedSecret(pod, secrets)\n        podTimestamp = getPodTimestamp(pod)\n\n        if pod.spec.volumes ~= nil then  \n          for _, volume in ipairs(pod.spec.volumes) do\n            if volume.secret ~= nil then\n              key = getKey(pod.metadata.namespace, volume.secret.secretName)\n              -- if secrets contains a certificate\n              if secrets[key] ~= nil then\n                if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                  return true, \"secret \" .. key .. \" has been updated after pod creation\"\n                end\n              end  \n            end\n\n            if volume.projected ~= nil and volume.projected.sources ~= nil then\n              for _, projectedResource in ipairs(volume.projected.sources) do\n                if projectedResource.secret ~= nil then\n                  key = getKey(pod.metadata.namespace, projectedResource.secret.name)\n                  -- if secrets contains a certificate\n                  if secrets[key] ~= nil then\n                    if isPodOlderThanSecret(podTimestamp, secrets[key]) then\n                      return true, \"secret \" .. key .. \" has been updated after pod creation\"\n                    end\n                  end  \n                end\n              end\n            end\n          end\n        end\n\n        return false\n      end      \n\n      function evaluate()\n        local hs = {}\n        hs.message = \"\"\n\n        local pods = {}\n        local certificates = {}\n        local secrets = {}\n\n        -- Contains all Secrets containing a Certificate generated using cert-manager\n        local certificateSecrets = {}\n\n        -- Contains all Secrets containing a Certificate generated using cert-manager\n        local degradedSecrets = {}\n\n        -- Separate secrets, pods and certificates\n        for _, resource in ipairs(resources) do\n          local kind = resource.kind\n          if kind == \"Certificate\" then\n            table.insert(certificates, resource)\n          elseif kind == \"Secret\" then\n            key = getKey(resource.metadata.namespace, resource.metadata.name)\n            updateTimestamp = getSecretUpdateTime(resource)\n            secrets[key] = updateTimestamp\n          elseif kind == \"Pod\" then\n            table.insert(pods, resource)\n          end\n        end\n\n        -- Find all secrets with certificate generated by cert-manager\n        for _, certificate in ipairs(certificates) do\n          key =  getKey(certificate.metadata.namespace, certificate.spec.secretName)\n          certificateSecrets[key] = secrets[key]\n        end\n\n        local podsWithOutdatedSecret = {}\n\n        for _, pod in ipairs(pods) do\n          outdatedData, message = hasOutdatedSecret(pod, certificateSecrets)\n          if outdatedData then\n            table.insert(podsWithOutdatedSecret, {resource= pod, message = message})\n          end\n        end\n\n        if #podsWithOutdatedSecret &gt; 0 then\n          hs.resources = podsWithOutdatedSecret\n        end\n        return hs\n      end\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/configmap/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/configmap/#introduction","title":"Introduction","text":"<p>There are many Kubernetes installations either as a manifest or a Helm chart that after undeployment leave <code>ConfigMap</code> behind.</p> <p>The below Cleaner instance finds all the <code>ConfigMaps</code> instances in all the namespaces which are orphaned (namespaces starting with <code>kube</code> are excluded)</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/configmap/#orphaned-configmap","title":"Orphaned ConfigMap","text":"<p>By orphaned we refer to a ConfigMap that is not used by:</p> <ul> <li>Pod through volumes (pod.spec.volumes)</li> <li>Pod through environment variables (pod.spec.containers.env and pod.spec.containers.envFrom)</li> </ul>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/configmap/#example-cleaner-instance","title":"Example - Cleaner Instance","text":"<pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: unused-configmaps\nspec:\n  schedule: \"* 0 * * *\"\n  action: Delete\n  resourcePolicySet:\n    resourceSelectors:\n    - kind: Pod\n      group: \"\"\n      version: v1\n    - kind: ConfigMap\n      group: \"\"\n      version: v1   \n    aggregatedSelection: |\n      function skipNamespace(namespace)\n        return string.match(namespace, '^kube')\n      end\n\n      function getKey(namespace, name)\n        return namespace .. \":\" .. name\n      end \n\n      function configMapsUsedByPods(pods)\n        local podConfigMaps = {}\n\n        for _, pod in ipairs(pods) do\n          if pod.spec.containers ~= nil then\n            for _, container in ipairs(pod.spec.containers) do\n\n              if container.env ~= nil then\n                for _, env in ipairs(container.env) do\n                  if env.valueFrom ~= nil and env.valueFrom.configMapKeyRef ~= nil then\n                    key = getKey(pod.metadata.namespace, env.valueFrom.configMapKeyRef.name)\n                    podConfigMaps[key] = true\n                  end\n                end\n              end\n\n              if container.envFrom ~= nil then\n                for _, envFrom in ipairs(container.envFrom) do\n                  if envFrom.configMapRef ~= nil then\n                    key = getKey(pod.metadata.namespace, envFrom.configMapRef.name)\n                    podConfigMaps[key] = true\n                  end\n                end\n              end  \n            end\n          end\n\n          if pod.spec.initContainers ~= nil then\n            for _, initContainer in ipairs(pod.spec.initContainers) do\n\n              if initContainer.env ~= nil then\n                for _, env in ipairs(initContainer.env) do\n                  if env.valueFrom ~= nil and env.valueFrom.configMapKeyRef ~= nil then\n                    key = getKey(pod.metadata.namespace, env.valueFrom.configMapKeyRef.name)\n                    podConfigMaps[key] = true\n                  end\n                end\n              end\n\n              if initContainer.envFrom ~= nil then\n                for _, envFrom in ipairs(initContainer.envFrom) do\n                  if envFrom.configMapRef ~= nil then\n                    key = getKey(pod.metadata.namespace,envFrom.configMapRef.name)\n                    podConfigMaps[key] = true\n                  end\n                end\n              end  \n\n            end\n          end    \n\n          if pod.spec.volumes ~= nil then  \n            for _, volume in ipairs(pod.spec.volumes) do\n              if volume.configMap ~= nil then\n                key = getKey(pod.metadata.namespace,volume.configMap.name)\n                podConfigMaps[key] = true\n              end\n\n              if volume.projected ~= nil and volume.projected.sources ~= nil then\n                for _, projectedResource in ipairs(volume.projected.sources) do\n                  if projectedResource.configMap ~= nil then\n                    key = getKey(pod.metadata.namespace,projectedResource.configMap.name)\n                    podConfigMaps[key] = true\n                  end\n                end\n              end\n            end\n          end\n        end  \n\n        return podConfigMaps\n      end\n\n      function evaluate()\n        local hs = {}\n        hs.message = \"\"\n\n        local pods = {}\n        local configMaps = {}\n        local unusedConfigMaps = {}\n\n        -- Separate configMaps and podsfrom the resources\n        for _, resource in ipairs(resources) do\n            local kind = resource.kind\n            if kind == \"ConfigMap\" and not skipNamespace(resource.metadata.namespace) then\n              table.insert(configMaps, resource)\n            elseif kind == \"Pod\" then\n              table.insert(pods, resource)\n            end\n        end\n\n        podConfigMaps = configMapsUsedByPods(pods)\n\n        for _, configMap in ipairs(configMaps) do\n          key = getKey(configMap.metadata.namespace,configMap.metadata.name)\n          if not podConfigMaps[key] then\n            table.insert(unusedConfigMaps, {resource = configMap})\n          end\n        end\n\n        if #unusedConfigMaps &gt; 0 then\n          hs.resources = unusedConfigMaps\n        end\n        return hs\n      end\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/jobs/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/jobs/#introduction","title":"Introduction","text":"<p>There are a number of Kubernetes installations that leave complete <code>Jobs</code> behind. This could be problematic as a new Helm chart installation might fail because completed <code>Jobs</code> are not cleanered from a previous uninstallation.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/jobs/#example-completed-jobs","title":"Example Completed Jobs","text":"<p>The Cleaner instance definition will find any <code>Jobs</code> with the below specifications.</p> <ul> <li><code>status.completionTime set</code></li> <li><code>status.succeeded</code> set to a value greater than zero</li> <li>The <code>Job</code> has no running or pending pods and will instruct Cleaner to delete it.</li> </ul> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: completed-jobs\nspec:\n  schedule: \"* 0 * * *\"\n  resourcePolicySet:\n    resourceSelectors:\n    - kind: Job\n      group: \"batch\"\n      version: v1\n      evaluate: |\n        function evaluate()\n          hs = {}\n          hs.matching = false\n          if obj.status ~= nil then\n            if obj.status.completionTime ~= nil and obj.status.succeeded &gt; 0 then\n              hs.matching = true\n            end\n          end\n          return hs\n        end\n  action: Delete\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/jobs/#example-long-running-pods","title":"Example - Long Running Pods","text":"<p>The Cleaner instance definition will find any <code>Pod</code> with the below specifications.</p> <ul> <li>Has been running for longer than one hour (3600 seconds)</li> <li>Was created by a Job</li> </ul> <p>The Cleaner instance will delete the Pod but do not the Job.</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\nname: pods-from-job\nspec:\nschedule: \"* 0 * * *\"\nresourcePolicySet:\n    resourceSelectors:\n    - kind: Pod\n    group: \"\"\n    version: v1\n    evaluate: |\n        --  Convert creationTimestamp \"2023-12-12T09:35:56Z\"\n        function convertTimestampString(timestampStr)\n        local convertedTimestamp = string.gsub(\n            timestampStr,\n            '(%d+)-(%d+)-(%d+)T(%d+):(%d+):(%d+)Z',\n            function(y, mon, d, h, mi, s)\n            return os.time({\n                year = tonumber(y),\n                month = tonumber(mon),\n                day = tonumber(d),\n                hour = tonumber(h),\n                min = tonumber(mi),\n                sec = tonumber(s)\n            })\n            end\n        )\n        return convertedTimestamp\n        end\n\n        function evaluate()\n        hs = {}\n        hs.matching = false\n\n        currentTime = os.time()\n\n        creationTimestamp = convertTimestampString(obj.metadata.creationTimestamp)\n\n        hs.message = creationTimestamp\n        print('creationTimestamp: ' .. creationTimestamp)\n        print('currentTime: ' .. currentTime)\n\n        timeDifference = os.difftime(currentTime, tonumber(creationTimestamp))\n\n        print('timeDifference: ' .. timeDifference)\n\n        -- if pod has been running for over an hour\n        if timeDifference &gt; 3600 then\n            if obj.metadata.ownerReferences ~= nil then\n            for _, owner in ipairs(obj.metadata.ownerReferences) do\n                if owner.kind == \"Job\" and owner.apiVersion == \"batch/v1\" then\n                hs.matching = true\n                end\n            end\n            end\n        end\n\n\n        return hs\n        end\naction: Delete\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/more_examples/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/more_examples/#more-examples","title":"More Examples","text":"<p>The k8s-cleaner has many more examples to explore here.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/more_examples/#contribute-to-k8s-cleaner-examples","title":"Contribute to k8s-cleaner Examples","text":"<p>We encourage all users to contribute to the example directory by adding their own Cleaner configurations\ud83d\udca1. This will help the community benefit from the expertise and build a stronger knowledge base of Cleaner use cases.</p> <p>To add an example, create a new file in the example directory with a descriptive name and add it in the Cleaner configuration. Once the example is added, feel free to submit a Pull Request (PR) to share it with the community.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/persistent_volume/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/persistent_volume/#introduction","title":"Introduction","text":"<p>The k8s-cleaner is able to delete unsused <code>PersistentVolumes</code>. The below Cleaner instance will find any <code>PersistentVolume</code> resources with the Phase set to anything but <code>Bound</code>.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/persistent_volume/#example-cleaner-instance","title":"Example - Cleaner Instance","text":"<pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: unbound-persistent-volumes\nspec:\n  schedule: \"* 0 * * *\"\n  resourcePolicySet:\n    resourceSelectors:\n    - kind: PersistentVolume\n      group: \"\"\n      version: v1\n      evaluate: |\n        function evaluate()\n          hs = {}\n          hs.matching = false\n          if obj.status ~= nil and obj.status.phase ~= \"Bound\" then\n            hs.matching = true\n          end\n          return hs\n        end\n  action: Delete\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/persistent_volume_claims/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/persistent_volume_claims/#introduction","title":"Introduction","text":"<p>The k8s-cleaner is able to delete unsused <code>PersistentVolumeClaim</code>. The below Cleaner instance will find any <code>PersistentVolumeClaim</code> resources which are currently unused by any Pods. The example below will consider all namespaces.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/persistent_volume_claims/#example-cleaner-instance","title":"Example - Cleaner Instance","text":"<pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: stale-persistent-volume-claim\nspec:\n  schedule: \"* 0 * * *\"\n  action: Delete # Delete matching resources\n  resourcePolicySet:\n    resourceSelectors:\n    - kind: Pod\n      group: \"\"\n      version: v1\n    - kind: PersistentVolumeClaim\n      group: \"\"\n      version: v1\n    aggregatedSelection: |\n      function isUsed(pvc, pods)\n        if pods == nil then\n          return false\n        end\n        for _, pod in ipairs(pods) do\n          if pod.spec.volumes ~= nil then\n            for _,volume in ipairs(pod.spec.volumes) do\n              if volume.persistentVolumeClaim ~= nil and volume.persistentVolumeClaim.claimName == pvc.metadata.name then\n                return true\n              end\n            end\n          end\n        end\n        return false\n      end  \n\n      function evaluate()\n        local hs = {}\n        hs.message = \"\"\n\n        local pods = {}\n        local pvcs = {}\n        local unusedPVCs = {}\n\n        -- Separate pods and pvcs from the resources\n        -- Group those by namespace\n        for _, resource in ipairs(resources) do\n          local kind = resource.kind\n          if kind == \"Pod\" then\n            if not pods[resource.metadata.namespace] then\n              pods[resource.metadata.namespace] = {}\n            end\n            table.insert(pods[resource.metadata.namespace], resource)\n          elseif kind == \"PersistentVolumeClaim\" then\n            if not pvcs[resource.metadata.namespace] then\n              pvcs[resource.metadata.namespace] = {}\n            end\n            table.insert(pvcs[resource.metadata.namespace], resource)\n          end\n        end\n\n        -- Iterate through each namespace and identify unused PVCs\n        for namespace, perNamespacePVCs in pairs(pvcs) do\n          for _, pvc in ipairs(perNamespacePVCs) do\n            if not isUsed(pvc, pods[namespace]) then\n              table.insert(unusedPVCs, {resource = pvc})\n            end\n          end\n        end\n\n        if #unusedPVCs &gt; 0 then\n          hs.resources = unusedPVCs\n        end\n        return hs\n      end\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/secret/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/secret/#introduction","title":"Introduction","text":"<p>There are many Kubernetes installations either as a manifest or a Helm chart that after undeployment leave Kubernetes <code>secret</code> behind.</p> <p>The below Cleaner instance finds all the <code>Secret</code> instances in all the namespaces which are orphaned.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/secret/#orphaned-secrets","title":"Orphaned Secrets","text":"<p>By orphaned we refer to a Secret that is not used by:</p> <ul> <li>Pod through volumes (pod.spec.volumes)</li> <li>Pod through environment variables (pod.spec.containers.env and pod.spec.containers.envFrom)</li> <li>Pod for image pulls (pod.spec.imagePullSecrets)</li> <li>Ingress TLS (ingress.spec.tls)</li> <li>ServiceAccounts</li> </ul>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/examples/unused_resources/secret/#example-cleaner-instance","title":"Example - Cleaner Instance","text":"<pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: unused-secrets\nspec:\n  schedule: \"* 0 * * *\"\n  action: Delete\n  resourcePolicySet:\n    resourceSelectors:\n    - kind: Pod\n      group: \"\"\n      version: v1\n    - kind: Secret\n      group: \"\"\n      version: v1\n    - kind: Ingress\n      group: \"networking.k8s.io\"\n      version: v1      \n    aggregatedSelection: |\n      function getKey(namespace, name)\n        return namespace .. \":\" .. name\n      end\n\n      function secretsUsedByPods(pods)\n        local podSecrets = {}\n\n        for _, pod in ipairs(pods) do\n          if pod.spec.containers ~= nil then\n            for _, container in ipairs(pod.spec.containers) do\n\n              if container.env ~= nil then\n                for _, env in ipairs(container.env) do\n                  if env.valueFrom ~= nil and env.valueFrom.secretKeyRef ~= nil then\n                    key = getKey(pod.metadata.namespace, env.valueFrom.secretKeyRef.name)\n                    podSecrets[key] = true\n                  end\n                end\n              end\n\n              if  container.envFrom ~= nil then\n                for _, envFrom in ipairs(container.envFrom) do\n                  if envFrom.secretRef ~= nil then\n                    key = getKey(pod.metadata.namespace, envFrom.secretRef.name)\n                    podSecrets[key] = true\n                  end\n                end\n              end  \n            end\n          end\n\n          if pod.spec.initContainers ~= nil then\n            for _, initContainer in ipairs(pod.spec.initContainers) do\n              if initContainer.env ~= nil then\n                for _, env in ipairs(initContainer.env) do\n                  if env.valueFrom ~= nil and env.valueFrom.secretKeyRef ~= nil then\n                    key = getKey(pod.metadata.namespace, env.valueFrom.secretKeyRef.name)\n                    podSecrets[key] = true\n                  end\n                end\n              end\n            end\n          end\n\n          if pod.spec.imagePullSecrets ~= nil then\n            for _, secret in ipairs(pod.spec.imagePullSecrets) do\n              key = getKey(pod.metadata.namespace, secret.name)\n              podSecrets[key] = true\n            end\n          end          \n\n          if pod.spec.volumes ~= nil then  \n            for _, volume in ipairs(pod.spec.volumes) do\n              if volume.secret ~= nil then\n                key = getKey(pod.metadata.namespace, volume.secret.secretName)\n                podSecrets[key] = true\n              end\n\n              if volume.projected ~= nil and volume.projected.sources ~= nil then\n                for _, projectedResource in ipairs(volume.projected.sources) do\n                  if projectedResource.secret ~= nil then\n                    key = getKey(pod.metadata.namespace, projectedResource.secret.name)\n                    podSecrets[key] = true\n                  end\n                end\n              end\n            end\n          end\n        end  \n\n        return podSecrets\n      end\n\n      function secretsUsedByIngresses(ingresses)\n        local ingressSecrets = {}\n        for _, ingress in ipairs(ingresses) do\n          if ingress.spec.tls ~= nil  then\n            for _, tls in ipairs(ingress.spec.tls) do\n              key = getKey(ingress.metadata.namespace, tls.secretName)\n              ingressSecrets[key] = true\n            end\n          end\n        end\n\n        return ingressSecrets\n      end\n\n      function evaluate()\n        local hs = {}\n        hs.message = \"\"\n\n        local pods = {}\n        local secrets = {}\n        local ingresses = {}\n        local unusedSecrets = {}\n\n        -- Separate secrets and pods and ingresses from the resources\n        for _, resource in ipairs(resources) do\n            local kind = resource.kind\n            if kind == \"Secret\" then\n              table.insert(secrets, resource)\n            elseif kind == \"Pod\" then\n              table.insert(pods, resource)\n            elseif kind == \"Ingress\" then\n              table.insert(ingresses, resource)\n            end\n        end\n\n        podSecrets = secretsUsedByPods(pods)\n        ingressSecrets = secretsUsedByIngresses(ingresses)\n\n        for _, secret in ipairs(secrets) do\n          if secret.type ~= \"kubernetes.io/service-account-token\" then\n            key = getKey(secret.metadata.namespace, secret.metadata.name)\n            if not podSecrets[key] and not ingressSecrets[key] then\n              table.insert(unusedSecrets, {resource = secret})\n            end\n          end\n        end\n\n        if #unusedSecrets &gt; 0 then\n          hs.resources = unusedSecrets\n        end\n        return hs\n      end\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/automated_operations/scale_up_down_resources/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/automated_operations/scale_up_down_resources/#manage-and-automate-resource-operations","title":"Manage and Automate Resource Operations","text":"<p>The k8s-cleaner does not just help users identify unused or unhealthy resources; it can also automate various operations to enhance the cluster's efficiency and management. For example, scale down Deployments/Daemonsets/Statefulsets based on a specified annotation.</p> <p>The example below demonstrates how to automatically scale down Deployments, DaemonSets, and StatefulSets with a specified annotation at a desired time (e.g., 8 PM nightly). Before scaling down, the replica count is stored in another annotation for later retrieval. At the configured scale-up time (e.g., 8 AM), resources are restored, ensuring efficient resource utilization during off-peak hours.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/automated_operations/scale_up_down_resources/#pause-yaml-definition","title":"Pause YAML Definition","text":"<p>Example - Pause</p> <pre><code># The defintion:\n# - runs at 8PM every day\n# - finds all Deployments/StatefulSet/DaemonSet with\n# annotation \"pause-resume\"\n#\n# For matched resources: \n# - stores current replicas in the annotation \"previous-replicas\"\n# - sets their replicas to zero (scale down and pause)\n#\n---\n    apiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: scale-down-deployment-statefulset-daemonset\nspec:\n  schedule: \"* 20 * * *\"\n  action: Transform\n  transform: |\n    -- Set replicas to 0\n    function transform()\n      hs = {}\n\n      if obj.metadata.annotations == nil then\n        obj.metadata.annotations = {}\n      end\n      -- store in the annotation current replicas value\n      obj.metadata.annotations[\"previous-replicas\"] = tostring(obj.spec.replicas)\n\n      -- reset replicas to 0\n      obj.spec.replicas = 0\n\n      hs.resource = obj\n      return hs\n    end  \n  resourcePolicySet:\n    resourceSelectors:\n    - kind: Deployment\n      group: apps\n      version: v1\n    - kind: StatefulSet\n      group: \"apps\"\n      version: v1\n    - kind: DaemonSet\n      group: \"apps\"\n      version: v1\n    aggregatedSelection: |\n      function evaluate()\n        local hs = {}\n\n        -- returns true if object has annotaiton \"pause-resume\" \n        function hasPauseAnnotation(obj)\n          if obj.metadata.annotations ~= nil then\n            if obj.metadata.annotations[\"pause-resume\"] then\n              return true\n            end\n\n            return false\n          end\n\n          return\n        end\n\n        local resourceToPause = {}\n\n        for _, resource in ipairs(resources) do\n          if hasPauseAnnotation(resource) then\n            table.insert(resourceToPause, {resource = resource})\n          end\n        end\n\n        if #resourceToPause &gt; 0 then\n          hs.resources = resourceToPause\n        end\n        return hs\n      end   \n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/automated_operations/scale_up_down_resources/#resume-yaml-definition","title":"Resume YAML Definition","text":"<p>As we defined the pause action for specific resources, during peak times, we would like to scale the Kubernetes resources back to their initial state. To do that, we will use the <code>resume</code> YAML definition found below.</p> <p>Example - Resume</p> <pre><code># The cleaner:\n# - runs at 8AM every day\n# - finds all Deployments/StatefulSet/DaemonSet with\n# annotation \"pause-resume\"\n#\n# For matched resources: \n# - gets the old replicas in the annotation \"previous-replicas\"\n# - sets the replicas to such value found above (scale deployment/statefulset/daemonset up)\n#\n---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: scale-up-deployment-statefulset-daemonset\nspec:\n  schedule: \"* 8 * * *\"\n  action: Transform\n  transform: |\n    -- Set replicas to 0\n    function transform()\n      hs = {}\n      if obj.metadata.annotations == nil then\n    return\n      end\n      if not obj.metadata.annotations[\"previous-replicas\"] then\n    return\n      end\n      -- reset replicas\n      obj.spec.replicas = tonumber(obj.metadata.annotations[\"previous-replicas\"])\n      hs.resource = obj\n      return hs\n    end  \n  resourcePolicySet:\n    resourceSelectors:\n    - kind: Deployment\n      group: apps\n      version: v1\n    - kind: StatefulSet\n      group: \"apps\"\n      version: v1\n    - kind: DaemonSet\n      group: \"apps\"\n      version: v1\n    aggregatedSelection: |\n      function evaluate()\n    local hs = {}\n\n    -- returns true if object has annotaiton \"pause-resume\" \n    function hasPauseAnnotation(obj)\n      if obj.metadata.annotations ~= nil then\n        if obj.metadata.annotations[\"pause-resume\"] then\n          return true\n        end\n\n        return false\n      end\n\n      return false\n    end\n\n    local resourceToUnPause = {}\n\n    for _, resource in ipairs(resources) do\n      if hasPauseAnnotation(resource) then\n        table.insert(resourceToUnPause, {resource = resource})\n      end\n    end\n\n    if #resourceToUnPause &gt; 0 then\n      hs.resources = resourceToUnPause\n    end\n    return hs\n      end   \n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/dryrun/dryrun/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/dryrun/dryrun/#introduction-to-dry-run","title":"Introduction to Dry Run","text":"<p>To preview which resources match the Cleaner's criteria, set the Action field to Scan. The k8s-cleaner will still execute its logic but will not delete and/or update any resources.</p> <p>To identify matching resources, you can then either ask k8s-cleaner to generate a Report or search the controller logs for the message <code>resource is a match for cleaner</code>.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/dryrun/dryrun/#example-dry-run","title":"Example - Dry Run","text":"<p>The example below, provides a definition of eliminating Deployments in the <code>test</code> namespace with both the <code>serving=api</code> and the <code>environment!=production</code> labels set. </p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-sample1\nspec:\n  schedule: \"* 0 * * *\" # Runs every day at midnight\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: test\n      kind: Deployment\n      group: \"apps\"\n      version: v1\n      labelFilters:\n      - key: serving\n        operation: Equal\n        value: api # Match deployments with the \"serving\" label set to \"api\"\n      - key: environment\n        operation: Different\n        value: production # Match deployments with the \"environment\" label different from \"production\"\n  action: Scan\n</code></pre> <p>By setting the Action field to Scan, we can safely test the Cleaner's filtering logic without affecting your actual deployment configurations. Once we are confident in the filtering criteria, you can set the Action to delete or modify.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/label_filters/label_filters/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/label_filters/label_filters/#introduction-to-label-filters","title":"Introduction to Label Filters","text":"<p>The k8s-cleaner has the ability to select resources based on a label. This capability allows precise resource management.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/label_filters/label_filters/#example-label-filters","title":"Example - Label Filters","text":"<p>The example below, provides a definition of eliminating Deployments in the <code>test</code> namespace with both the <code>serving=api</code> and the <code>environment!=production</code> labels set. </p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-sample1\nspec:\n  schedule: \"* 0 * * *\" # Executes every day at midnight\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: test\n      kind: Deployment\n      group: \"apps\"\n      version: v1\n      labelFilters:\n      - key: serving\n        operation: Equal\n        value: api # Identifies Deployments with \"serving\" label set to \"api\"\n      - key: environment\n        operation: Different\n        value: production # Identifies Deployments with \"environment\" label different from \"production\"\n  action: Delete # Deletes matching Deployments\n</code></pre> <p>By utilising the label filters capability, we can refine the scope of resource management, ensuring that only specific resources are targeted for removal and/or update. </p> <p>This approach helps maintain a clean and organised Kubernetes environment without affecting unintended resources.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/resourceselector/resourceselector/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/resourceselector/resourceselector/#introduction-to-resourceselector","title":"Introduction to resourceSelector","text":"<p>It might be cases that operator need to examine resources of distinct types simultaneously.</p> <p>Let's assume we would like to eliminate all Deployment instances that are not backed-up by an Autoscaler instance. The k8s-cleaner allows this action. By employing the <code>resourceSelector</code>, we can select all <code>Deployment</code> and <code>Autoscaler</code> instances.</p> <p>As a next step, we have to define the <code>aggregatedSelection</code>. <code>AggregatedSelection</code> will be given all instances collected by the Cleaner using the <code>resourceSelector</code>. In this example, all Deployment and Autoscaler instances in the foo namespace.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/resourceselector/resourceselector/#example-deployment-not-backed-up-by-autoscaler","title":"Example  - Deployment not Backed-up by Autoscaler","text":"<pre><code>---\n# Find all Deployments not backed up by an Autoscaler. Those are a match.\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-sample3\nspec:\n  schedule: \"* 0 * * *\"\n  action: Delete # Delete matching resources\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: foo\n      kind: Deployment\n      group: \"apps\"\n      version: v1\n    - namespace: foo\n      kind: HorizontalPodAutoscaler\n      group: \"autoscaling\"\n      version: v2\n    aggregatedSelection: |\n      function evaluate()\n        local hs = {}\n        hs.valid = true\n        hs.message = \"\"\n\n        local deployments = {}\n        local autoscalers = {}\n        local deploymentWithNoAutoscaler = {}\n\n        -- Separate deployments and services from the resources\n        for _, resource in ipairs(resources) do\n            local kind = resource.kind\n                if kind == \"Deployment\" then\n                    table.insert(deployments, resource)\n                elseif kind == \"HorizontalPodAutoscaler\" then\n                    table.insert(autoscalers, resource)\n                end\n        end\n\n        -- Check for each deployment if there is a matching HorizontalPodAutoscaler\n        for _, deployment in ipairs(deployments) do\n            local deploymentName = deployment.metadata.name\n            local matchingAutoscaler = false\n\n            for _, autoscaler in ipairs(autoscalers) do\n                if autoscaler.spec.scaleTargetRef.name == deployment.metadata.name then\n                    matchingAutoscaler = true\n                    break\n                end\n            end\n\n            if not matchingAutoscaler then\n                table.insert(deploymentWithNoAutoscaler, {resource = deployment})\n                break\n            end\n        end\n\n        if #deploymentWithNoAutoscaler &gt; 0 then\n          hs.resources = deploymentWithNoAutoscaler\n        end\n        return hs\n      end\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/schedule/schedule/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/schedule/schedule/#introduction-to-schedule","title":"Introduction to Schedule","text":"<p>The schedule field specifies when the k8s-cleaner should run its logic to identify and potentially delete or update matching resources.</p> <p>It adheres to the <code>Cron syntax</code>, which is a widely adopted scheduling language for tasks and events.</p> <p>The Cron syntax consists of five fields, separated by spaces, each representing a specific part of the scheduling period: minute, hour, day of month, month and day of week, in that order.</p> <p>The k8s-cleaner is able to accept the below schedule formats.</p> <ul> <li>Standard crontab specs, e.g. \"* * * * ?\"</li> <li>Descriptors, e.g. \"@midnight\", \"@every 1h30m\"</li> </ul>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/schedule/schedule/#example","title":"Example","text":"<pre><code>---\n# This Cleaner instance finds any Jobs that:\n# - have status.completionTime set\n# - have status.succeeded set to a value greater than zero\n# - have no running or pending pods\n# Cleaner will delete the resources every 1h30m.\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: completed-jobs\nspec:\n  schedule: \"@every 1h30m\"\n  resourcePolicySet:\n    resourceSelectors:\n    - kind: Job\n        group: \"batch\"\n        version: v1\n        evaluate: |\n          function evaluate()\n            hs = {}\n            hs.matching = false\n            if obj.status ~= nil then\n              if obj.status.completionTime ~= nil and obj.status.succeeded &gt; 0 then\n                hs.matching = true\n              end\n            end\n            return hs\n          end\n  action: Delete    \n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/schedule/schedule/#validation","title":"Validation","text":"<pre><code>$ kubectl apply -f \"cleaner.yaml\" \ncleaner.apps.projectsveltos.io/completed-jobs created\n\n$ kubectl get cleaner -n projectsveltos\nNAME             AGE\ncompleted-jobs   7s\n\n$ kubectl get cleaner completed-jobs -n projectsveltos -o yaml\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"apps.projectsveltos.io/v1alpha1\",\"kind\":\"Cleaner\",\"metadata\":{\"annotations\":{},\"name\":\"completed-jobs\"},\"spec\":{\"action\":\"Delete\",\"resourcePolicySet\":{\"resourceSelectors\":[{\"evaluate\":\"function evaluate()\\n  hs = {}\\n  hs.matching = false\\n  if obj.status ~= nil then\\n    if obj.status.completionTime ~= nil and obj.status.succeeded \\u003e 0 then\\n      hs.matching = true\\n    end\\n  end\\n  return hs\\nend\\n\",\"group\":\"batch\",\"kind\":\"Job\",\"version\":\"v1\"}]},\"schedule\":\"@every 1h30m\"}}\n  creationTimestamp: \"2024-08-03T14:09:49Z\"\n  finalizers:\n  - projectsveltos.io/cleaner-finalizer\n  generation: 1\n  name: completed-jobs\n  resourceVersion: \"11334709\"\n  uid: 205c20e1-cb3a-43ee-96f9-9ccb2b2cdf40\nspec:\n  action: Delete\n  resourcePolicySet:\n    resourceSelectors:\n    - evaluate: |\n        function evaluate()\n          hs = {}\n          hs.matching = false\n          if obj.status ~= nil then\n            if obj.status.completionTime ~= nil and obj.status.succeeded &gt; 0 then\n              hs.matching = true\n            end\n          end\n          return hs\n        end\n      group: batch\n      kind: Job\n      version: v1\n  schedule: '@every 1h30m'\nstatus:\n  nextScheduleTime: \"2024-08-03T15:39:49Z\"\n</code></pre> <p>From the above example, we can observe the next schedule to be nextScheduleTime: \"2024-08-03T15:39:49Z\".</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/store_resources/store_resource_yaml/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/store_resources/store_resource_yaml/#store-resource-yaml","title":"Store Resource Yaml","text":"<p>This is a small section describing how to store resources before the k8s-cleaner deletes or modifies them. The k8s-cleaner has an optional field called <code>StoreResourcePath</code>.</p> <p>When this option is set, the k8s-cleaner will dump all the maching resources before any modification (update and/or deletion) is performed.</p> <p>The maching resource will be stored in the below directory.</p> <pre><code>/&lt;__StoreResourcePath__ value&gt;/&lt;Cleaner name&gt;/&lt;resourceNamespace&gt;/&lt;resource Kind&gt;/&lt;resource Name&gt;.yaml\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/store_resources/store_resource_yaml/#example-unsused-configmap","title":"Example - Unsused ConfigMap","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/store_resources/store_resource_yaml/#step-1-create-persistentvolumeclaim","title":"Step 1 - Create PersistentVolumeClaim","text":"<p>PersistentVolumeClaim</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cleaner-pvc\n  namespace: projectsveltos\n  labels:\n    app: k8s-cleaner\nspec:\n  storageClassName: standard\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 2Gi\n</code></pre> <p>The above YAML definition will create a <code>PersistentVolumeClaim</code> of 2Gi. In case more storage is required, simply update the YAML definition.</p> <pre><code>$ kubectl apply -f \"pvc.yaml\"\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/store_resources/store_resource_yaml/#step-2-update-k8s-cleaner-controller-deployment","title":"Step 2 - Update k8s-cleaner-controller Deployment","text":"<p>The next is to update the <code>k8s-cleaner-controller</code> deployment located in the <code>projectsveltos</code> namespace. Then, we will define the <code>PersistentVolumeClaim</code> and the actual storage location.</p> <pre><code>$ kubectl get deploy -n projectsveltos\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\nk8s-cleaner-controller   1/1     1            1           10m\n$ kubectl edit deploy k8s-cleaner-controller -n projectsveltos\n</code></pre> <p>k8s-cleaner-controller</p> <pre><code>volumes:\n- name: volume\n  persistentVolumeClaim:\n    claimName: cleaner-pvc\n\n  volumeMounts:\n  - mountPath: /pvc/\n    name: volume\n</code></pre> <p>The YAML defition files will be stored in <code>/pvc/</code>.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/store_resources/store_resource_yaml/#step-3-cleaner-resource-creation","title":"Step 3 - Cleaner Resource Creation","text":"<p>In step 3, we will create a Cleaner Resource and define the deletion of any unused <code>configMap</code> resources based on a cron job. To store the resources before performing any deletions, we will add the argument <code>storeResourcePath: \"/pvc/\"</code> and store the resources inside the <code>/pvc/</code> directory.</p> <p>For instance get this Cleaner instance that finds unused ConfigMaps. Set spec.storeResourcePath: \"/pvc/\" (eventually change spec.action: Scan)</p> <p>When cleaner find the ununsed <code>ConfigMap</code>, it will first store the resource definition and then delete the actual resource.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/store_resources/store_resource_yaml/#validation","title":"Validation","text":"<pre><code>docker exec -i cleaner-management-worker ls /var/local-path-provisioner/pvc-8314c600-dc54-4e23-a796-06b73080f589_projectsveltos_cleaner-pvc\nunused-configmaps\n\n/var/local-path-provisioner/pvc-8314c600-dc54-4e23-a796-06b73080f589_projectsveltos_cleaner-pvc/unused-configmaps/test/ConfigMap:\nkube-root-ca.crt.yaml\nmy-configmap.yaml\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/update_resources/update_resources/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/update_resources/update_resources/#introduction-to-resource-update","title":"Introduction to Resource Update","text":"<p>Beyond removing stale resources, the k8s-cleaner can also facilitate in the dynamic update of existing resource configurations.</p> <p>The capability allows users to modify resource specifications based on specific criteria, ensuring alignment with evolving requirements and maintaining resource consistency.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/features/update_resources/update_resources/#example-resource-update","title":"Example - Resource Update","text":"<p>Consider the scenario where we want to update Service objects in the <code>foo</code> namespace to use version2 apps.</p> <ol> <li>The evaluate function allows users to select resources, Services in the <code>foo</code> namespace pointing to version1 apps.</li> <li>The trasnform function will change the matching resources, by updating the <code>obj.spec.selector[\"app\"]</code> to version2.</li> </ol> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-sample3\nspec:\n  schedule: \"* 0 * * *\"\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: foo\n      kind: Service\n      group: \"\"\n      version: v1\n      evaluate: |\n        -- Define how resources will be selected \n        function evaluate()\n        hs = {}\n        hs.matching = false\n        if obj.spec.selector ~= nil then\n          if obj.spec.selector[\"app\"] == \"version1\" then\n            hs.matching = true\n          end\n        end\n        return hs\n        end\n  action: Transform # Update matching resources\n  transform: |\n      -- Define how resources will be updated\n      function transform()\n      hs = {}\n      obj.spec.selector[\"app\"] = \"version2\"\n      hs.resource = obj\n      return hs\n      end\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install/#what-is-k8s-cleaner","title":"What is k8s-cleaner?","text":"<p>The Kubernetes controller Cleaner (k8s-cleaner) identifies, removes, or updates stale/orphaned or unhealthy resources to maintain a clean and efficient Kubernetes cluster.</p> <p>It is designed to handle any Kubernetes resource types (including custom Kubernetes resources) and provides sophisticated filtering capabilities, including label-based selection and custom Lua-based criteria.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install/#pre-requisites","title":"Pre-requisites","text":"<p>To work with the k8s-cleaner, ensure you have the below points covered.</p> <ol> <li>A Kubernetes cluster</li> <li>kubectl CLI installed</li> <li>kubeconfig for authentication</li> </ol>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install/#installation","title":"Installation","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install/#kubernetes-manifest","title":"Kubernetes Manifest","text":"<p>The k8s-cleaner can be installed in any Kubernetes cluster independent if it is in an on-prem or in a Cloud environment. The k8s-cleaner can be deployed down the clusters with your favourite Continious Deployment tool! The installation is pretty simple.</p> <pre><code>$ export KUBECONFIG=&lt;directory to the kubeconfig file&gt;\n\n$ kubectl apply -f https://raw.githubusercontent.com/gianlucam76/k8s-cleaner/v0.17.1/manifest/manifest.yaml\n</code></pre> <p>Note</p> <p>The above command will create a new namespace with the name <code>projectsveltos</code> and install the Kubernetes cleaner controller there.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install/#helm-chart","title":"Helm Chart","text":"<p>There is the option to install the k8s-cleaner with a Helm chart. To do so, simply follow the commands listed below.</p> <pre><code>$ helm install k8s-cleaner oci://ghcr.io/gianlucam76/charts/k8s-cleaner \\\n    --version 0.17.1 \\\n    --namespace k8s-cleaner \\\n    --create-namespace #(1)\n</code></pre> <ol> <li>It will create the namespace k8s-cleaner and deploy everything in the namespace</li> </ol>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install/#validation","title":"Validation","text":"<pre><code>$ kubectl get namespace\nNAME              STATUS   AGE\ndefault           Active   6h11m\nk8s-cleaner       Active   34s\nkube-node-lease   Active   6h11m\nkube-public       Active   6h11m\nkube-system       Active   6h11m\n\n$ kubectl get all -n k8s-cleaner\nNAME                               READY   STATUS    RESTARTS   AGE\npod/k8s-cleaner-78b9d794c5-jpp76   2/2     Running   0          43s\n\nNAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/k8s-cleaner-metrics   ClusterIP   10.43.149.237   &lt;none&gt;        8081/TCP   43s\n\nNAME                          READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/k8s-cleaner   1/1     1            1           43s\n\nNAME                                     DESIRED   CURRENT   READY   AGE\nreplicaset.apps/k8s-cleaner-78b9d794c5   1         1         1       43s\n</code></pre> <p>Tip</p> <p>Before getting started with the k8s-cleaner, have a look at the Features section. Familiarise with the label filters, store resource, update resources, resource selector, and schedule sections. Use the examples provided and familiarise with the syntax and the capabilities provided</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install_on_multiple_cluster/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"<p>Sveltos is a set of Kubernetes controllers that run in the management cluster. From the management cluster, Sveltos can manage add-ons and applications on a fleet of managed Kubernetes clusters. It is a declarative tool to ensure that the desired state of an application is always reflected in the actual state of the Kubernetes managed clusters.</p> <p>In a management cluster, each individual Kubernetes cluster is represented by a dedicated resource. Labels can be attached to those resources.</p> <p>Sveltos configuration utilises a concept called a cluster selector. This selector essentially acts like a filter based on Kubernetes labels. By defining specific labels or combinations of labels, you can create a subset of clusters that share those characteristics.</p> <p></p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install_on_multiple_cluster/#deploying-k8s-cleaner-across-kubernetes-clusters-with-sveltos","title":"Deploying k8s-cleaner across Kubernetes Clusters with Sveltos","text":"<p>By combining k8s-cleaner with Sveltos, you gain a powerful solution for maintaining clean, efficient, and secure Kubernetes clusters across your entire fleet. This approach simplifies management, reduces operational overhead, and optimizes resource utilization.</p> <p></p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install_on_multiple_cluster/#step-1-install-sveltos-on-management-cluster","title":"Step 1: Install Sveltos on Management Cluster","text":"<p>For this demonstration, we will install Sveltos in the management cluster. Sveltos installation details can be found here.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/v0.43.0/manifest/manifest.yaml\nkubectl apply -f https://raw.githubusercontent.com/projectsveltos/sveltos/v0.43.0/manifest/default-classifier.yaml\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install_on_multiple_cluster/#step-2-register-civo-cluster-with-sveltos","title":"Step 2: Register Civo Cluster with Sveltos","text":"<p>Create two Kubernetes clusters using Civo UI. Download the Kubeconfigs, then:</p> <pre><code>kubectl create ns civo\nsveltosctl register cluster --namespace=civo --cluster=cluster1 --kubeconfig=civo-cluster1-kubeconfig --labels=env=fv\nsveltosctl register cluster --namespace=civo --cluster=cluster2 --kubeconfig=civo-cluster2-kubeconfig --labels=env=fv\n</code></pre> <p>Verify your Civo were successfully registered:</p> <pre><code>kubectl get sveltoscluster -n civo\nNAME       READY   VERSION\ncluster1   true    v1.29.2+k3s1\ncluster2   true    v1.28.7+k3s1 \n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/install_on_multiple_cluster/#step-3-create-deployment-configuration","title":"Step 3: Create Deployment Configuration","text":"<p>This guide demonstrates how to deploy k8s-cleaner across all your clusters labeled with <code>env=fv</code> using Sveltos.  This helps optimize resource utilization by removing unused configurations and ensuring Pods use the latest secret data.</p> <p>Applying this configuration will trigger Sveltos to deploy k8s-cleaner to all managed clusters.</p> <pre><code>apiVersion: config.projectsveltos.io/v1beta1\nkind: ClusterProfile\nmetadata:\n  name: deploy-k8s-cleaner\nspec:\n  clusterSelector:\n    matchLabels:\n      env: fv\n  syncMode: Continuous\n  helmCharts:\n  - repositoryURL:    oci://ghcr.io/gianlucam76/charts\n    repositoryName:   k8s-cleaner\n    chartName:        k8s-cleaner\n    chartVersion:     0.10.0\n    releaseName:      k8s-cleaner\n    releaseNamespace: k8s-cleaner\n    helmChartAction:  Install\n</code></pre> <ul> <li>unused-configmaps.yaml: contains a Cleaner that identifies and removes unused ConfigMaps to optimize resource utilization.</li> <li>pod-with-outdated-secret-data.yaml: contains a Cleaner instance that detects Pods using outdated Secrets and triggers necessary actions (e.g., restart) to ensure up-to-date data.</li> <li>clusterprofile-deploy-cleaner.yaml: instructs Sveltos to deploy Cleaner instances across all managed clusters.</li> </ul> <pre><code>kubectl apply -f https://raw.githubusercontent.com/projectsveltos/demos/main/k8s-cleaner/unused-configmaps.yaml\nkubectl apply -f https://raw.githubusercontent.com/projectsveltos/demos/main/k8s-cleaner/pod-with-outdated-secret-data.yaml\nkubectl apply -f https://raw.githubusercontent.com/projectsveltos/demos/main/k8s-cleaner/clusterprofile-deploy-cleaner.yaml\n</code></pre> <p>Using sveltosctl we can verify all resources have been deployed.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/telemetry/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/telemetry/#telemetry","title":"Telemetry","text":"<p>As an open-source project, k8s-cleaner relies on user insights to guide its development. Telemetry data helps us:</p> <ul> <li>Prioritize Features: Identify the most commonly used features and focus on enhancing them.</li> <li>Improve Performance: Analyze usage patterns to optimize Sveltos' performance and resource utilization.</li> <li>Make Informed Decisions: Use data-driven insights to shape the future of Sveltos.</li> </ul> <p>By choosing to participate in telemetry, users contribute to the ongoing improvement of Sveltos and help ensure it meets the needs of the community.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/telemetry/#what-data-do-we-collect","title":"What Data Do We Collect?","text":"<p>We collect minimal, anonymized data about Sveltos usage:</p> <ul> <li>Version Information: To track the distribution of different k8s-cleaner versions.</li> <li>Cluster Management Data: To understand the scale and complexity of Sveltos deployments. This includes:<ol> <li>Number of Cleaner instances</li> <li>Number of nodes</li> </ol> </li> </ul>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/telemetry/#how-we-protect-your-privacy","title":"How We Protect Your Privacy","text":"<ul> <li>Anonymized Data: All data is collected and processed anonymously, without any personally identifiable information.</li> <li>Secure Storage: Telemetry data is stored securely and access is strictly controlled by the Sveltos maintainers.</li> </ul>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/telemetry/#opting-out-of-telemetry","title":"Opting-Out of Telemetry","text":"<p>To opt-out from the telemetry data set the <code>--disable-telemetry=true</code> flag in the k8s-cleaner deployment.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"getting_started/install/telemetry/#requesting-data-erasure","title":"Requesting Data Erasure","text":"<p>You have the right to request the erasure of your data under certain circumstances. To initiate a data erasure request, please contact us at support@projectsveltos.io.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#introduction-to-notifications","title":"Introduction to Notifications","text":"<p>Notifications is an easy way of k8s-cleaner to keep users in the loop about relevant updates. Each notification contains a list of successfully deleted or modified resources by the k8s-cleaner.</p> <p>The below notifications are available. - Slack - Webex - Discord - Telegram - Teams - SMTP - Kubernetes Event</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#slack-notifications-example","title":"Slack Notifications Example","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#kubernetes-secret","title":"Kubernetes Secret","text":"<p>To allow the k8s-cleaner to write messages or upload files to a channel, we need to create a Kubernetes secret</p> <pre><code>$ kubectl create secret generic slack --from-literal=SLACK_TOKEN=&lt;YOUR TOKEN&gt; --from-literal=SLACK_CHANNEL_ID=&lt;YOUR CHANNEL ID&gt;\n</code></pre> <p>Slack Notifications Defintion</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-with-slack-notifications\nspec:\n  schedule: \"0 * * * *\"\n  action: Delete # Delete matching resources\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: test\n      kind: Deployment\n      group: \"apps\"\n      version: v1\n  notifications:\n  - name: slack\n    type: Slack\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: slack\n      namespace: default\n</code></pre> <p>Anytime this Cleaner instance is processed, a Slack message is sent containing all the resources that were deleted by k8s-cleaner.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#webex-notifications-example","title":"Webex Notifications Example","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#kubernetes-secret_1","title":"Kubernetes Secret","text":"<p>To allow the k8s-cleaner to write messages or upload files to a channel, we need to create a Kubernetes secret</p> <pre><code>$ kubectl create secret generic webex --from-literal=WEBEX_TOKEN=&lt;YOUR TOKEN&gt; --from-literal=WEBEX_ROOM_ID=&lt;YOUR WEBEX CHANNEL ID&gt;\n</code></pre> <p>Webex Notifications Defintion</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-with-webex-notifications\nspec:\n  schedule: \"0 * * * *\"\n  action: Delete # Delete matching resources\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: test\n      kind: Deployment\n      group: \"apps\"\n      version: v1\n  notifications:\n  - name: webex\n    type: Webex\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: webex\n      namespace: default\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#discord-notifications-example","title":"Discord Notifications Example","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#kubernetes-secret_2","title":"Kubernetes Secret","text":"<p>To allow the k8s-cleaner to write messages or upload files to a channel, we need to create a Kubernetes secret</p> <pre><code>$ kubectl create secret generic discord --from-literal=DISCORD_TOKEN=&lt;YOUR TOKEN&gt; --from-literal=DISCORD_CHANNEL_ID=&lt;YOUR DISCORD CHANNEL ID&gt;\n</code></pre> <p>Discord Notifications Defintion</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-with-discord-notifications\nspec:\n  schedule: \"0 * * * *\"\n  action: Delete # Delete matching resources\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: test\n      kind: Deployment\n      group: \"apps\"\n      version: v1\n  notifications:\n  - name: discord\n    type: Discord\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: discord\n      namespace: default\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#telegram-notifications-example","title":"Telegram Notifications Example","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#kubernetes-secret_3","title":"Kubernetes Secret","text":"<p>To allow the k8s-cleaner to write messages or upload files to a group, we need to create a Kubernetes secret</p> <pre><code>$ kubectl create secret generic telegram --from-literal=TELEGRAM_TOKEN=&lt;YOUR TOKEN&gt; --from-literal=TELEGRAM_CHAT_ID=&lt;YOUR TELEGRAM CHAT ID&gt;\n</code></pre> <p>Telegram Notifications Defintion</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-with-telegram-notifications\nspec:\n  schedule: \"0 * * * *\"\n  action: Delete # Delete matching resources\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: test\n      kind: Deployment\n      group: \"apps\"\n      version: v1\n  notifications:\n  - name: telegram\n    type: Telegram\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: telegram\n      namespace: default\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#teams-notifications-example","title":"Teams Notifications Example","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#kubernetes-secret_4","title":"Kubernetes Secret","text":"<p>To allow the k8s-cleaner to write messages or upload files to a channel, we need to create a Kubernetes secret</p> <pre><code>$ kubectl create secret generic teams --from-literal=TEAMS_WEBHOOK_URL=\"&lt;your URL&gt;\"\n</code></pre> <p>Teams Notifications Defintion</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-with-teams-notifications\nspec:\n  schedule: \"0 * * * *\"\n  action: Delete # Delete matching resources\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: test\n      kind: Deployment\n      group: \"apps\"\n      version: v1\n  notifications:\n  - name: teams\n    type: Teams\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: teams\n      namespace: default\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#smtp-notifications-example","title":"SMTP Notifications Example","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#kubernetes-secret_5","title":"Kubernetes Secret","text":"<p>To allow the k8s-cleaner to send an SMTP email, we need to create a Kubernetes secret:</p> <pre><code>$ kubectl create secret generic smtp \\\n  --from-literal=SMTP_RECIPIENTS=&lt;COMMA-SEPARATED EMAIL ADDRESSES&gt; \\\n  --from-literal=SMTP_BCC=&lt;OPTIONAL, COMMA-SEPARATED EMAIL ADDRESSES&gt; \\\n  --from-literal=SMTP_IDENTITY=&lt;OPTIONAL, IDENTITY/USERNAME OF THE SENDER&gt; \\\n  --from-literal=SMTP_SENDER=&lt;EMAIL ADDRESS&gt; \\\n  --from-literal=SMTP_PASSWORD=&lt;OPTIONAL, SMTP PASSWORD FOR EMAIL ADDRESS IF APPLICABLE&gt; \\\n  --from-literal=SMTP_HOST=&lt;SMTP SERVER HOSTNAME&gt; \\\n  --from-literal=SMTP_PORT=&lt;OPTIONAL, SMTP SERVER PORT, DEFAULTS TO \"587\"&gt;\n</code></pre> <p>SMTP Notifications Definition</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-with-smtp-notifications\nspec:\n  schedule: \"0 * * * *\"\n  action: Delete # Delete matching resources\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: test\n      kind: Deployment\n      group: \"apps\"\n      version: v1\n  notifications:\n  - name: smtp\n    type: SMTP\n    notificationRef:\n      apiVersion: v1\n      kind: Secret\n      name: smtp\n      namespace: default\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"notifications/notifications/#kubernetes-event-notifications-example","title":"Kubernetes Event Notifications Example","text":"<p>To allow the k8s-cleaner to generate a Kubernetes event for each matching resource</p> <p>Kubernetes Event Notifications Definition</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-with-event-notifications\nspec:\n  schedule: \"0 * * * *\"\n  action: Scan # Scan matching resources\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: test\n      kind: Deployment\n      group: \"apps\"\n      version: v1\n  notifications:\n  - name: event\n    type: Event\n</code></pre> <p>Cleaner will generate a Kubernetes Event for each Deployment matching this Cleaner instance</p> <p><code>` 20m (x2 over 56m)   Normal   k8s-cleaner   Deployment/nginx   [ns:nginx] resource matching Cleaner instance cleaner-with-event-notifications (current action Scan)</code></p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"reports/k8s-cleaner_reports/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"reports/k8s-cleaner_reports/#introduction-to-k8s-cleaner-reports","title":"Introduction to k8s-cleaner Reports","text":"<p>Users have the ability to instruct the k8s-cleaner to generate a report with all the resources deleted or modified.</p> <p>The k8s-cleaner will create a Report instance based on the name of the Report name.</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"reports/k8s-cleaner_reports/#example-report-defintion","title":"Example - Report Defintion","text":"<p>Cleaner Definition with Notifications set to type CleanerReport</p> <pre><code>---\napiVersion: apps.projectsveltos.io/v1alpha1\nkind: Cleaner\nmetadata:\n  name: cleaner-with-report\nspec:\n  schedule: \"0 * * * *\"\n  action: Delete # Delete matching resources\n  resourcePolicySet:\n    resourceSelectors:\n    - namespace: test\n      kind: Deployment\n      group: \"apps\"\n      version: v1\n  notifications:\n  - name: report\n    type: CleanerReport\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"reports/k8s-cleaner_reports/#validation","title":"Validation","text":"<pre><code>$ kubectl get report           \nNAME              AGE\ncleaner-sample3   51m\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"reports/k8s-cleaner_reports/#report-output","title":"Report Output","text":"<pre><code>apiVersion: apps.projectsveltos.io/v1alpha1\nkind: Report\nmetadata:\n  creationTimestamp: \"2023-12-17T17:05:00Z\"\n  generation: 2\n  name: cleaner-with-report\n  resourceVersion: \"1625\"\n  uid: dda9a231-9a51-4133-aeb5-f0520feb8746\nspec:\n  action: Delete\n  message: 'time: 2023-12-17 17:07:00.394736089 +0000 UTC m=+129.172023518'\n  resources:\n  - apiVersion: apps/v1\n    kind: Deployment\n    name: my-nginx-deployment\n    namespace: test\n</code></pre>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"resources/blogs/","title":"k8s-cleaner - Kubernetes Controller that identifies, removes, or updates stale/orphaned or unhealthy resources","text":"<p>Star</p>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]},{"location":"resources/blogs/#k8s-cleaner-available-articles","title":"k8s-cleaner Available Articles","text":"<ol> <li>Automated Kubernetes Cluster Cleanup at Scale</li> </ol>","tags":["Kubernetes","Controller","Kubernetes Resources","Identify","Update","Remove"]}]}